{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82649fed",
   "metadata": {
    "papermill": {
     "duration": 0.015268,
     "end_time": "2021-11-01T17:15:34.696633",
     "exception": false,
     "start_time": "2021-11-01T17:15:34.681365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tabular Playground Series - Nov 21\n",
    "\n",
    "This month, our data consists of 284 feature variables and our target variable is binary classification. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601d754",
   "metadata": {
    "papermill": {
     "duration": 0.011766,
     "end_time": "2021-11-01T17:15:34.720654",
     "exception": false,
     "start_time": "2021-11-01T17:15:34.708888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports \n",
    "\n",
    "Let's import some of the libraries we will be using throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37721aab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-01T17:15:34.756907Z",
     "iopub.status.busy": "2021-11-01T17:15:34.756038Z",
     "iopub.status.idle": "2021-11-01T17:15:41.819453Z",
     "shell.execute_reply": "2021-11-01T17:15:41.818800Z",
     "shell.execute_reply.started": "2021-11-01T16:32:12.568831Z"
    },
    "papermill": {
     "duration": 7.08625,
     "end_time": "2021-11-01T17:15:41.819599",
     "exception": false,
     "start_time": "2021-11-01T17:15:34.733349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv\n",
      "/kaggle/input/tabular-playground-series-nov-2021/train.csv\n",
      "/kaggle/input/tabular-playground-series-nov-2021/test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Import on Kaggle\n",
    "import os\n",
    "import time\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Importing processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing Visualisation libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing libraries for the metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "# Keras Imports\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Importing libraries for the model\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# sklearn imports for analysis\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3638cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:15:41.851379Z",
     "iopub.status.busy": "2021-11-01T17:15:41.850651Z",
     "iopub.status.idle": "2021-11-01T17:16:06.592058Z",
     "shell.execute_reply": "2021-11-01T17:16:06.592488Z",
     "shell.execute_reply.started": "2021-11-01T16:32:15.745191Z"
    },
    "papermill": {
     "duration": 24.759516,
     "end_time": "2021-11-01T17:16:06.592675",
     "exception": false,
     "start_time": "2021-11-01T17:15:41.833159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\n",
    "test_data = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcfff83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:06.622419Z",
     "iopub.status.busy": "2021-11-01T17:16:06.621856Z",
     "iopub.status.idle": "2021-11-01T17:16:06.797612Z",
     "shell.execute_reply": "2021-11-01T17:16:06.798066Z",
     "shell.execute_reply.started": "2021-11-01T16:32:45.193209Z"
    },
    "papermill": {
     "duration": 0.192131,
     "end_time": "2021-11-01T17:16:06.798244",
     "exception": false,
     "start_time": "2021-11-01T17:16:06.606113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1d696",
   "metadata": {
    "papermill": {
     "duration": 0.0128,
     "end_time": "2021-11-01T17:16:06.824093",
     "exception": false,
     "start_time": "2021-11-01T17:16:06.811293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Memory Reduction\n",
    "\n",
    "If you don't have any issues with memory, you can go ahead and skip this step. \n",
    "Here, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. \n",
    "\n",
    "There are several other methods to save RAM - you can refer to this article on [14 tips to save RAM memory](https://www.kaggle.com/pavansanagapati/14-simple-tips-to-save-ram-memory-for-1-gb-dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250f15e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:06.863855Z",
     "iopub.status.busy": "2021-11-01T17:16:06.862012Z",
     "iopub.status.idle": "2021-11-01T17:16:06.866969Z",
     "shell.execute_reply": "2021-11-01T17:16:06.867388Z",
     "shell.execute_reply.started": "2021-11-01T16:32:45.379230Z"
    },
    "papermill": {
     "duration": 0.030274,
     "end_time": "2021-11-01T17:16:06.867540",
     "exception": false,
     "start_time": "2021-11-01T17:16:06.837266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage of features: \n",
      " Index    0.000122\n",
      "f0       4.577637\n",
      "f1       4.577637\n",
      "f2       4.577637\n",
      "f3       4.577637\n",
      "f4       4.577637\n",
      "f5       4.577637\n",
      "dtype: float64\n",
      "memory usage sum:  462.3414306640625\n"
     ]
    }
   ],
   "source": [
    "memory_usage = data.memory_usage(deep=True) / 1024 ** 2\n",
    "print('memory usage of features: \\n', memory_usage.head(7))\n",
    "print('memory usage sum: ',memory_usage.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ffdc21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:06.907805Z",
     "iopub.status.busy": "2021-11-01T17:16:06.907153Z",
     "iopub.status.idle": "2021-11-01T17:16:21.405854Z",
     "shell.execute_reply": "2021-11-01T17:16:21.405371Z",
     "shell.execute_reply.started": "2021-11-01T16:32:45.408672Z"
    },
    "papermill": {
     "duration": 14.525344,
     "end_time": "2021-11-01T17:16:21.406013",
     "exception": false,
     "start_time": "2021-11-01T17:16:06.880669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 115.01 Mb (75.1% reduction)\n",
      "Mem. usage decreased to 105.06 Mb (74.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "data = reduce_memory_usage(data, verbose=True)\n",
    "test_data = reduce_memory_usage(test_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d5285a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:21.444779Z",
     "iopub.status.busy": "2021-11-01T17:16:21.443369Z",
     "iopub.status.idle": "2021-11-01T17:16:30.780549Z",
     "shell.execute_reply": "2021-11-01T17:16:30.780072Z",
     "shell.execute_reply.started": "2021-11-01T16:33:00.010318Z"
    },
    "papermill": {
     "duration": 9.360741,
     "end_time": "2021-11-01T17:16:30.780676",
     "exception": false,
     "start_time": "2021-11-01T17:16:21.419935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.0000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>600000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.506010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.796875</td>\n",
       "      <td>-1.223633</td>\n",
       "      <td>-1843.0000</td>\n",
       "      <td>-1.368164</td>\n",
       "      <td>-3.207031</td>\n",
       "      <td>-1.169922</td>\n",
       "      <td>-1.059570</td>\n",
       "      <td>-1.282227</td>\n",
       "      <td>-1.242188</td>\n",
       "      <td>-2.578125</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.217773</td>\n",
       "      <td>-9.765625</td>\n",
       "      <td>-4.667969</td>\n",
       "      <td>-3.101562</td>\n",
       "      <td>-1.276367</td>\n",
       "      <td>-1.584961</td>\n",
       "      <td>-1.254883</td>\n",
       "      <td>-3.994141</td>\n",
       "      <td>-2.783203</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.026215</td>\n",
       "      <td>1.186523</td>\n",
       "      <td>43.5625</td>\n",
       "      <td>1.442383</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>1.385742</td>\n",
       "      <td>1.333984</td>\n",
       "      <td>1.291992</td>\n",
       "      <td>0.019562</td>\n",
       "      <td>...</td>\n",
       "      <td>1.213867</td>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.024490</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>1.248047</td>\n",
       "      <td>1.347656</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.097778</td>\n",
       "      <td>2.515625</td>\n",
       "      <td>133.6250</td>\n",
       "      <td>2.634766</td>\n",
       "      <td>0.061584</td>\n",
       "      <td>2.589844</td>\n",
       "      <td>2.800781</td>\n",
       "      <td>2.558594</td>\n",
       "      <td>2.476562</td>\n",
       "      <td>0.058746</td>\n",
       "      <td>...</td>\n",
       "      <td>2.386719</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.063416</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>2.601562</td>\n",
       "      <td>2.681641</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>0.058472</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.397217</td>\n",
       "      <td>3.787109</td>\n",
       "      <td>302.2500</td>\n",
       "      <td>3.908203</td>\n",
       "      <td>0.112732</td>\n",
       "      <td>3.814453</td>\n",
       "      <td>3.996094</td>\n",
       "      <td>3.824219</td>\n",
       "      <td>3.804688</td>\n",
       "      <td>0.101074</td>\n",
       "      <td>...</td>\n",
       "      <td>3.693359</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>0.088135</td>\n",
       "      <td>0.113098</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>3.820312</td>\n",
       "      <td>3.839844</td>\n",
       "      <td>0.110718</td>\n",
       "      <td>0.104858</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.781250</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6120.0000</td>\n",
       "      <td>6.519531</td>\n",
       "      <td>8.265625</td>\n",
       "      <td>6.515625</td>\n",
       "      <td>6.585938</td>\n",
       "      <td>6.257812</td>\n",
       "      <td>6.390625</td>\n",
       "      <td>7.078125</td>\n",
       "      <td>...</td>\n",
       "      <td>6.574219</td>\n",
       "      <td>18.406250</td>\n",
       "      <td>10.210938</td>\n",
       "      <td>8.625000</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>6.253906</td>\n",
       "      <td>6.144531</td>\n",
       "      <td>10.765625</td>\n",
       "      <td>5.988281</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  f0             f1           f2             f3  \\\n",
       "count  600000.000000  600000.000000  600000.0000  600000.000000   \n",
       "mean             NaN            NaN          NaN            NaN   \n",
       "std         0.000000       0.000000          NaN       0.000000   \n",
       "min        -3.796875      -1.223633   -1843.0000      -1.368164   \n",
       "25%         0.026215       1.186523      43.5625       1.442383   \n",
       "50%         0.097778       2.515625     133.6250       2.634766   \n",
       "75%         0.397217       3.787109     302.2500       3.908203   \n",
       "max         8.781250       6.226562    6120.0000       6.519531   \n",
       "\n",
       "                  f4             f5             f6             f7  \\\n",
       "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
       "mean             NaN            NaN            NaN            NaN   \n",
       "std         0.000000       0.000000       0.000000       0.000000   \n",
       "min        -3.207031      -1.169922      -1.059570      -1.282227   \n",
       "25%         0.019714       1.260742       1.385742       1.333984   \n",
       "50%         0.061584       2.589844       2.800781       2.558594   \n",
       "75%         0.112732       3.814453       3.996094       3.824219   \n",
       "max         8.265625       6.515625       6.585938       6.257812   \n",
       "\n",
       "                  f8             f9  ...            f91            f92  \\\n",
       "count  600000.000000  600000.000000  ...  600000.000000  600000.000000   \n",
       "mean             NaN            NaN  ...            NaN            NaN   \n",
       "std         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "min        -1.242188      -2.578125  ...      -1.217773      -9.765625   \n",
       "25%         1.291992       0.019562  ...       1.213867       0.018906   \n",
       "50%         2.476562       0.058746  ...       2.386719       0.068909   \n",
       "75%         3.804688       0.101074  ...       3.693359       0.125122   \n",
       "max         6.390625       7.078125  ...       6.574219      18.406250   \n",
       "\n",
       "                 f93            f94            f95            f96  \\\n",
       "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
       "mean        0.059357            NaN       0.106689            NaN   \n",
       "std         0.119446       0.000000       0.209106       0.000000   \n",
       "min        -4.667969      -3.101562      -1.276367      -1.584961   \n",
       "25%         0.024490       0.017059       0.025467       1.248047   \n",
       "50%         0.056641       0.063416       0.062164       2.601562   \n",
       "75%         0.088135       0.113098       0.101990       3.820312   \n",
       "max        10.210938       8.625000       3.656250       6.253906   \n",
       "\n",
       "                 f97            f98            f99         target  \n",
       "count  600000.000000  600000.000000  600000.000000  600000.000000  \n",
       "mean             NaN            NaN            NaN       0.506010  \n",
       "std         0.000000       0.000000       0.000000       0.499964  \n",
       "min        -1.254883      -3.994141      -2.783203       0.000000  \n",
       "25%         1.347656       0.013535       0.018112       0.000000  \n",
       "50%         2.681641       0.058044       0.058472       1.000000  \n",
       "75%         3.839844       0.110718       0.104858       1.000000  \n",
       "max         6.144531      10.765625       5.988281       1.000000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa86d98",
   "metadata": {
    "papermill": {
     "duration": 0.014131,
     "end_time": "2021-11-01T17:16:30.809520",
     "exception": false,
     "start_time": "2021-11-01T17:16:30.795389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sampling Data\n",
    "\n",
    "Now that we have reduced the memory usage by over 70%, let's sample the data. \n",
    "\n",
    "Why are we doing this? Well, you don't have to. But if you're like me and own a Macbook Air that can't handle a dataset bigger than 100mb, this might be a good idea.\n",
    "\n",
    "When we are performing model selection and hyperparameter tuning later, we can't afford to let the notebook run for hours on end testing every model. Doing this, preserves the distributions of each feature while taking only 20% of the entire dataset and we can reduce the training time by using this sampled data.\n",
    "\n",
    "We can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data. Once we decide on the model we want to use and improve its performance, we can train the final model on the entire dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce776f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:30.844764Z",
     "iopub.status.busy": "2021-11-01T17:16:30.843930Z",
     "iopub.status.idle": "2021-11-01T17:16:30.945385Z",
     "shell.execute_reply": "2021-11-01T17:16:30.945821Z",
     "shell.execute_reply.started": "2021-11-01T16:33:09.800397Z"
    },
    "papermill": {
     "duration": 0.122094,
     "end_time": "2021-11-01T17:16:30.945991",
     "exception": false,
     "start_time": "2021-11-01T17:16:30.823897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = data.sample(int(len(data) * 0.2))\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "220f9617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:30.979078Z",
     "iopub.status.busy": "2021-11-01T17:16:30.978178Z",
     "iopub.status.idle": "2021-11-01T17:16:31.340061Z",
     "shell.execute_reply": "2021-11-01T17:16:31.339557Z",
     "shell.execute_reply.started": "2021-11-01T16:33:09.906511Z"
    },
    "papermill": {
     "duration": 0.379643,
     "end_time": "2021-11-01T17:16:31.340199",
     "exception": false,
     "start_time": "2021-11-01T17:16:30.960556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's confirm if the sampling is retaining the feature distributions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.histplot(\n",
    "    data=data, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n",
    ")\n",
    "sns.histplot(\n",
    "    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa260b75",
   "metadata": {
    "papermill": {
     "duration": 0.015647,
     "end_time": "2021-11-01T17:16:31.371813",
     "exception": false,
     "start_time": "2021-11-01T17:16:31.356166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb1abe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:31.406837Z",
     "iopub.status.busy": "2021-11-01T17:16:31.405593Z",
     "iopub.status.idle": "2021-11-01T17:16:31.408274Z",
     "shell.execute_reply": "2021-11-01T17:16:31.408775Z",
     "shell.execute_reply.started": "2021-11-01T16:33:10.352159Z"
    },
    "papermill": {
     "duration": 0.021456,
     "end_time": "2021-11-01T17:16:31.408937",
     "exception": false,
     "start_time": "2021-11-01T17:16:31.387481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features = data.columns\n",
    "# scale = MinMaxScaler()\n",
    "# sample_df[features]=scale.fit_transform(sample_df[features])\n",
    "# sample_df[features]= scale.transform(sample_df[features])  \n",
    "\n",
    "# print('Data scaled using : ', scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63611abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:31.445493Z",
     "iopub.status.busy": "2021-11-01T17:16:31.442575Z",
     "iopub.status.idle": "2021-11-01T17:16:31.464299Z",
     "shell.execute_reply": "2021-11-01T17:16:31.463804Z",
     "shell.execute_reply.started": "2021-11-01T16:33:10.358070Z"
    },
    "papermill": {
     "duration": 0.039773,
     "end_time": "2021-11-01T17:16:31.464436",
     "exception": false,
     "start_time": "2021-11-01T17:16:31.424663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = sample_df.drop('target', axis=1)\n",
    "y = sample_df.target\n",
    "del sample_df # we do this to remove sample_df from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580362df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:31.500448Z",
     "iopub.status.busy": "2021-11-01T17:16:31.498070Z",
     "iopub.status.idle": "2021-11-01T17:16:31.757044Z",
     "shell.execute_reply": "2021-11-01T17:16:31.756455Z",
     "shell.execute_reply.started": "2021-11-01T16:51:09.298212Z"
    },
    "papermill": {
     "duration": 0.277338,
     "end_time": "2021-11-01T17:16:31.757199",
     "exception": false,
     "start_time": "2021-11-01T17:16:31.479861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = test_data.drop('id', axis=1)\n",
    "tt = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4550f5a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:31.794033Z",
     "iopub.status.busy": "2021-11-01T17:16:31.793194Z",
     "iopub.status.idle": "2021-11-01T17:16:31.933483Z",
     "shell.execute_reply": "2021-11-01T17:16:31.932993Z",
     "shell.execute_reply.started": "2021-11-01T17:03:18.340970Z"
    },
    "papermill": {
     "duration": 0.161033,
     "end_time": "2021-11-01T17:16:31.933621",
     "exception": false,
     "start_time": "2021-11-01T17:16:31.772588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_DUPLICATE_LIB_OK=True\n",
      "   KMP_INIT_AT_FORK=FALSE\n",
      "   KMP_SETTINGS=1\n",
      "   KMP_WARNINGS=0\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=true\n",
      "   KMP_ENABLE_TASK_THROTTLING=true\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=4\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=false\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=1\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED: deprecated; max-active-levels-var=1\n",
      "   OMP_NUM_THREADS: value is not defined\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n",
      "2021-11-01 17:16:31.834513: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(100,)),\n",
    "    Dense(100, activation=tf.nn.swish),\n",
    "    Dense(64, activation=tf.nn.swish),\n",
    "    Dense(32, activation=tf.nn.swish),\n",
    "    Dense(16, activation=tf.nn.swish),\n",
    "    Dense(1, activation=tf.nn.sigmoid),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647968e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:16:31.974566Z",
     "iopub.status.busy": "2021-11-01T17:16:31.968960Z",
     "iopub.status.idle": "2021-11-01T17:20:45.564871Z",
     "shell.execute_reply": "2021-11-01T17:20:45.565813Z",
     "shell.execute_reply.started": "2021-11-01T17:08:52.642021Z"
    },
    "papermill": {
     "duration": 253.616693,
     "end_time": "2021-11-01T17:20:45.566268",
     "exception": false,
     "start_time": "2021-11-01T17:16:31.949575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "CV 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 17:16:32.460387: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "47/47 [==============================] - 2s 20ms/step - loss: 0.8714 - auc: 0.5113 - val_loss: 0.6928 - val_auc: 0.5420\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6841 - auc: 0.5848 - val_loss: 0.6719 - val_auc: 0.6300\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6520 - auc: 0.6722 - val_loss: 0.6364 - val_auc: 0.7021\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6320 - auc: 0.7075 - val_loss: 0.6271 - val_auc: 0.7179\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6265 - auc: 0.7172 - val_loss: 0.6199 - val_auc: 0.7275\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6183 - auc: 0.7285 - val_loss: 0.6162 - val_auc: 0.7327\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6177 - auc: 0.7286 - val_loss: 0.6457 - val_auc: 0.7094\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6249 - auc: 0.7217 - val_loss: 0.6127 - val_auc: 0.7362\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6124 - auc: 0.7343 - val_loss: 0.6111 - val_auc: 0.7373\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6087 - auc: 0.7387 - val_loss: 0.6119 - val_auc: 0.7346\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6111 - auc: 0.7354 - val_loss: 0.6119 - val_auc: 0.7361\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6066 - auc: 0.7403 - val_loss: 0.6086 - val_auc: 0.7378\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6098 - auc: 0.7347 - val_loss: 0.6118 - val_auc: 0.7367\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.6049 - auc: 0.7395 - val_loss: 0.6134 - val_auc: 0.7363\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5979 - auc: 0.7455 - val_loss: 0.6084 - val_auc: 0.7339\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5980 - auc: 0.7443 - val_loss: 0.6282 - val_auc: 0.7189\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5995 - auc: 0.7426 - val_loss: 0.6053 - val_auc: 0.7346\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5991 - auc: 0.7428 - val_loss: 0.6035 - val_auc: 0.7437\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5926 - auc: 0.7483 - val_loss: 0.5994 - val_auc: 0.7403\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5923 - auc: 0.7479 - val_loss: 0.5988 - val_auc: 0.7399\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5902 - auc: 0.7491 - val_loss: 0.5964 - val_auc: 0.7443\n",
      "Epoch 22/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5857 - auc: 0.7528 - val_loss: 0.5971 - val_auc: 0.7424\n",
      "Epoch 23/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5905 - auc: 0.7495 - val_loss: 0.5954 - val_auc: 0.7434\n",
      "Epoch 24/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5845 - auc: 0.7544 - val_loss: 0.6121 - val_auc: 0.7338\n",
      "Epoch 25/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5902 - auc: 0.7498 - val_loss: 0.6162 - val_auc: 0.7303\n",
      "Epoch 26/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5882 - auc: 0.7512 - val_loss: 0.5915 - val_auc: 0.7446\n",
      "Epoch 27/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5807 - auc: 0.7574 - val_loss: 0.6012 - val_auc: 0.7378\n",
      "Epoch 28/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5812 - auc: 0.7568 - val_loss: 0.6052 - val_auc: 0.7373\n",
      "Epoch 29/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5899 - auc: 0.7509 - val_loss: 0.5946 - val_auc: 0.7447\n",
      "Epoch 30/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5790 - auc: 0.7591 - val_loss: 0.5889 - val_auc: 0.7465\n",
      "Epoch 31/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5793 - auc: 0.7593 - val_loss: 0.5905 - val_auc: 0.7451\n",
      "Epoch 32/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5772 - auc: 0.7612 - val_loss: 0.5960 - val_auc: 0.7435\n",
      "Epoch 33/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5765 - auc: 0.7615 - val_loss: 0.5880 - val_auc: 0.7452\n",
      "Epoch 34/1000\n",
      "47/47 [==============================] - 1s 22ms/step - loss: 0.5769 - auc: 0.7619 - val_loss: 0.5952 - val_auc: 0.7406\n",
      "Epoch 35/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5740 - auc: 0.7642 - val_loss: 0.5899 - val_auc: 0.7456\n",
      "Epoch 36/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5721 - auc: 0.7656 - val_loss: 0.5899 - val_auc: 0.7449\n",
      "Epoch 37/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5715 - auc: 0.7667 - val_loss: 0.5895 - val_auc: 0.7451\n",
      "Epoch 38/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5711 - auc: 0.7675 - val_loss: 0.5876 - val_auc: 0.7459\n",
      "Epoch 39/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5676 - auc: 0.7701 - val_loss: 0.5932 - val_auc: 0.7426\n",
      "Epoch 40/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5680 - auc: 0.7702 - val_loss: 0.6075 - val_auc: 0.7379\n",
      "Epoch 41/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5693 - auc: 0.7692 - val_loss: 0.5975 - val_auc: 0.7404\n",
      "Epoch 42/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5698 - auc: 0.7694 - val_loss: 0.5956 - val_auc: 0.7445\n",
      "Epoch 43/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5734 - auc: 0.7669 - val_loss: 0.5969 - val_auc: 0.7427\n",
      "Epoch 44/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5662 - auc: 0.7718 - val_loss: 0.5997 - val_auc: 0.7409\n",
      "Epoch 45/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5672 - auc: 0.7716 - val_loss: 0.5906 - val_auc: 0.7449\n",
      "Epoch 46/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5571 - auc: 0.7798 - val_loss: 0.5880 - val_auc: 0.7464\n",
      "Epoch 47/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5554 - auc: 0.7810 - val_loss: 0.5888 - val_auc: 0.7459\n",
      "Epoch 48/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5548 - auc: 0.7815 - val_loss: 0.5892 - val_auc: 0.7455\n",
      "Epoch 49/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5546 - auc: 0.7817 - val_loss: 0.5888 - val_auc: 0.7454\n",
      "Epoch 50/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5542 - auc: 0.7824 - val_loss: 0.5889 - val_auc: 0.7454\n",
      "Epoch 51/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5539 - auc: 0.7826 - val_loss: 0.5880 - val_auc: 0.7455\n",
      "Epoch 52/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5533 - auc: 0.7830 - val_loss: 0.5890 - val_auc: 0.7459\n",
      "Epoch 53/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5519 - auc: 0.7842 - val_loss: 0.5887 - val_auc: 0.7450\n",
      "Epoch 54/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5513 - auc: 0.7846 - val_loss: 0.5886 - val_auc: 0.7452\n",
      "Epoch 55/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5512 - auc: 0.7848 - val_loss: 0.5889 - val_auc: 0.7446\n",
      "Epoch 56/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5512 - auc: 0.7848 - val_loss: 0.5881 - val_auc: 0.7453\n",
      "Epoch 57/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5509 - auc: 0.7849 - val_loss: 0.5884 - val_auc: 0.7451\n",
      "Epoch 58/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5507 - auc: 0.7851 - val_loss: 0.5883 - val_auc: 0.7452\n",
      "Fold 1 NN: 0.74592\n",
      "Training fold 2\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "47/47 [==============================] - 2s 18ms/step - loss: 0.6285 - auc: 0.7219 - val_loss: 0.5862 - val_auc: 0.7595\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5875 - auc: 0.7544 - val_loss: 0.5825 - val_auc: 0.7625\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.5844 - auc: 0.7566 - val_loss: 0.5765 - val_auc: 0.7636\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5818 - auc: 0.7585 - val_loss: 0.5779 - val_auc: 0.7618\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.5792 - auc: 0.7605 - val_loss: 0.5804 - val_auc: 0.7616\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5780 - auc: 0.7617 - val_loss: 0.5807 - val_auc: 0.7590\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5750 - auc: 0.7636 - val_loss: 0.5760 - val_auc: 0.7630\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5721 - auc: 0.7666 - val_loss: 0.5770 - val_auc: 0.7610\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5704 - auc: 0.7673 - val_loss: 0.5761 - val_auc: 0.7602\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5718 - auc: 0.7666 - val_loss: 0.5798 - val_auc: 0.7611\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.5682 - auc: 0.7701 - val_loss: 0.5881 - val_auc: 0.7572\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5694 - auc: 0.7697 - val_loss: 0.5736 - val_auc: 0.7627\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5677 - auc: 0.7708 - val_loss: 0.5733 - val_auc: 0.7631\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5658 - auc: 0.7718 - val_loss: 0.5790 - val_auc: 0.7603\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5635 - auc: 0.7741 - val_loss: 0.5841 - val_auc: 0.7556\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5655 - auc: 0.7734 - val_loss: 0.5739 - val_auc: 0.7624\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5631 - auc: 0.7752 - val_loss: 0.5799 - val_auc: 0.7605\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5603 - auc: 0.7772 - val_loss: 0.5766 - val_auc: 0.7591\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5606 - auc: 0.7778 - val_loss: 0.5860 - val_auc: 0.7579\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5627 - auc: 0.7763 - val_loss: 0.5846 - val_auc: 0.7562\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5516 - auc: 0.7854 - val_loss: 0.5756 - val_auc: 0.7606\n",
      "Epoch 22/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5488 - auc: 0.7872 - val_loss: 0.5746 - val_auc: 0.7609\n",
      "Epoch 23/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5490 - auc: 0.7877 - val_loss: 0.5746 - val_auc: 0.7613\n",
      "Epoch 24/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.5480 - auc: 0.7877 - val_loss: 0.5751 - val_auc: 0.7608\n",
      "Epoch 25/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5475 - auc: 0.7883 - val_loss: 0.5750 - val_auc: 0.7606\n",
      "Epoch 26/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5470 - auc: 0.7888 - val_loss: 0.5754 - val_auc: 0.7605\n",
      "Epoch 27/1000\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.5474 - auc: 0.7888 - val_loss: 0.5770 - val_auc: 0.7594\n",
      "Epoch 28/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5453 - auc: 0.7904 - val_loss: 0.5752 - val_auc: 0.7606\n",
      "Epoch 29/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5450 - auc: 0.7907 - val_loss: 0.5754 - val_auc: 0.7606\n",
      "Epoch 30/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5449 - auc: 0.7906 - val_loss: 0.5753 - val_auc: 0.7603\n",
      "Epoch 31/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5449 - auc: 0.7908 - val_loss: 0.5759 - val_auc: 0.7598\n",
      "Epoch 32/1000\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.5448 - auc: 0.7908 - val_loss: 0.5759 - val_auc: 0.7604\n",
      "Epoch 33/1000\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.5446 - auc: 0.7911 - val_loss: 0.5754 - val_auc: 0.7605\n",
      "Fold 2 NN: 0.76317\n",
      "Training fold 3\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "47/47 [==============================] - 2s 18ms/step - loss: 0.5883 - auc: 0.7563 - val_loss: 0.5856 - val_auc: 0.7621\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5732 - auc: 0.7665 - val_loss: 0.5671 - val_auc: 0.7706\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5673 - auc: 0.7707 - val_loss: 0.5754 - val_auc: 0.7679\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5690 - auc: 0.7697 - val_loss: 0.5725 - val_auc: 0.7690\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5707 - auc: 0.7678 - val_loss: 0.5710 - val_auc: 0.7669\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.5614 - auc: 0.7756 - val_loss: 0.5680 - val_auc: 0.7696\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5663 - auc: 0.7723 - val_loss: 0.5717 - val_auc: 0.7672\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5648 - auc: 0.7743 - val_loss: 0.5794 - val_auc: 0.7611\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5605 - auc: 0.7775 - val_loss: 0.5766 - val_auc: 0.7637\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5537 - auc: 0.7831 - val_loss: 0.5655 - val_auc: 0.7701\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5513 - auc: 0.7847 - val_loss: 0.5658 - val_auc: 0.7700\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5508 - auc: 0.7853 - val_loss: 0.5672 - val_auc: 0.7697\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.5503 - auc: 0.7856 - val_loss: 0.5650 - val_auc: 0.7701\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5496 - auc: 0.7862 - val_loss: 0.5649 - val_auc: 0.7700\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5494 - auc: 0.7865 - val_loss: 0.5663 - val_auc: 0.7701\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5507 - auc: 0.7856 - val_loss: 0.5663 - val_auc: 0.7697\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5485 - auc: 0.7873 - val_loss: 0.5659 - val_auc: 0.7697\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5486 - auc: 0.7871 - val_loss: 0.5657 - val_auc: 0.7696\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5485 - auc: 0.7873 - val_loss: 0.5654 - val_auc: 0.7699\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5472 - auc: 0.7885 - val_loss: 0.5661 - val_auc: 0.7698\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5484 - auc: 0.7877 - val_loss: 0.5663 - val_auc: 0.7692\n",
      "Epoch 22/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5459 - auc: 0.7897 - val_loss: 0.5660 - val_auc: 0.7695\n",
      "Epoch 23/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5455 - auc: 0.7899 - val_loss: 0.5658 - val_auc: 0.7696\n",
      "Epoch 24/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5454 - auc: 0.7901 - val_loss: 0.5659 - val_auc: 0.7696\n",
      "Epoch 25/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5452 - auc: 0.7903 - val_loss: 0.5664 - val_auc: 0.7695\n",
      "Epoch 26/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5453 - auc: 0.7903 - val_loss: 0.5663 - val_auc: 0.7692\n",
      "Epoch 27/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.5451 - auc: 0.7903 - val_loss: 0.5662 - val_auc: 0.7693\n",
      "Epoch 28/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5451 - auc: 0.7905 - val_loss: 0.5660 - val_auc: 0.7696\n",
      "Epoch 29/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5447 - auc: 0.7908 - val_loss: 0.5660 - val_auc: 0.7695\n",
      "Epoch 30/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5446 - auc: 0.7909 - val_loss: 0.5661 - val_auc: 0.7694\n",
      "Epoch 31/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5447 - auc: 0.7909 - val_loss: 0.5660 - val_auc: 0.7695\n",
      "Epoch 32/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5446 - auc: 0.7909 - val_loss: 0.5661 - val_auc: 0.7694\n",
      "Epoch 33/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5446 - auc: 0.7909 - val_loss: 0.5660 - val_auc: 0.7695\n",
      "Epoch 34/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5446 - auc: 0.7909 - val_loss: 0.5660 - val_auc: 0.7693\n",
      "Fold 3 NN: 0.77004\n",
      "Training fold 4\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "47/47 [==============================] - 2s 18ms/step - loss: 0.5732 - auc: 0.7670 - val_loss: 0.5539 - val_auc: 0.7821\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5616 - auc: 0.7760 - val_loss: 0.5642 - val_auc: 0.7765\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5661 - auc: 0.7736 - val_loss: 0.5623 - val_auc: 0.7775\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5581 - auc: 0.7792 - val_loss: 0.5553 - val_auc: 0.7811\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5627 - auc: 0.7765 - val_loss: 0.5599 - val_auc: 0.7774\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5610 - auc: 0.7779 - val_loss: 0.5596 - val_auc: 0.7782\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.5576 - auc: 0.7807 - val_loss: 0.5655 - val_auc: 0.7749\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5567 - auc: 0.7814 - val_loss: 0.5645 - val_auc: 0.7749\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5477 - auc: 0.7887 - val_loss: 0.5565 - val_auc: 0.7789\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5450 - auc: 0.7904 - val_loss: 0.5570 - val_auc: 0.7786\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5464 - auc: 0.7895 - val_loss: 0.5574 - val_auc: 0.7780\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5444 - auc: 0.7909 - val_loss: 0.5570 - val_auc: 0.7785\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5438 - auc: 0.7917 - val_loss: 0.5575 - val_auc: 0.7781\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5440 - auc: 0.7913 - val_loss: 0.5564 - val_auc: 0.7790\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5434 - auc: 0.7919 - val_loss: 0.5590 - val_auc: 0.7776\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5419 - auc: 0.7932 - val_loss: 0.5567 - val_auc: 0.7785\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 0.5411 - auc: 0.7937 - val_loss: 0.5568 - val_auc: 0.7785\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5411 - auc: 0.7936 - val_loss: 0.5568 - val_auc: 0.7784\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5411 - auc: 0.7940 - val_loss: 0.5569 - val_auc: 0.7782\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5410 - auc: 0.7938 - val_loss: 0.5568 - val_auc: 0.7785\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5409 - auc: 0.7939 - val_loss: 0.5566 - val_auc: 0.7785\n",
      "Fold 4 NN: 0.78201\n",
      "Training fold 5\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "47/47 [==============================] - 2s 18ms/step - loss: 0.5882 - auc: 0.7570 - val_loss: 0.5526 - val_auc: 0.7854\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5612 - auc: 0.7755 - val_loss: 0.5519 - val_auc: 0.7849\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5583 - auc: 0.7785 - val_loss: 0.5526 - val_auc: 0.7845\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5592 - auc: 0.7783 - val_loss: 0.5541 - val_auc: 0.7831\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5564 - auc: 0.7805 - val_loss: 0.5636 - val_auc: 0.7787\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5558 - auc: 0.7810 - val_loss: 0.5715 - val_auc: 0.7698\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.5584 - auc: 0.7801 - val_loss: 0.5560 - val_auc: 0.7803\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 0.5545 - auc: 0.7825 - val_loss: 0.5618 - val_auc: 0.7769\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5544 - auc: 0.7826 - val_loss: 0.5641 - val_auc: 0.7759\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5473 - auc: 0.7889 - val_loss: 0.5513 - val_auc: 0.7828\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5441 - auc: 0.7912 - val_loss: 0.5529 - val_auc: 0.7825\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5445 - auc: 0.7911 - val_loss: 0.5506 - val_auc: 0.7833\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5433 - auc: 0.7918 - val_loss: 0.5530 - val_auc: 0.7819\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5429 - auc: 0.7923 - val_loss: 0.5542 - val_auc: 0.7811\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5429 - auc: 0.7925 - val_loss: 0.5517 - val_auc: 0.7826\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5418 - auc: 0.7934 - val_loss: 0.5534 - val_auc: 0.7815\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5418 - auc: 0.7933 - val_loss: 0.5533 - val_auc: 0.7815\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.5414 - auc: 0.7937 - val_loss: 0.5538 - val_auc: 0.7806\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5413 - auc: 0.7941 - val_loss: 0.5530 - val_auc: 0.7817\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5394 - auc: 0.7955 - val_loss: 0.5524 - val_auc: 0.7820\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5391 - auc: 0.7958 - val_loss: 0.5522 - val_auc: 0.7818\n",
      "Epoch 22/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5392 - auc: 0.7959 - val_loss: 0.5522 - val_auc: 0.7819\n",
      "Epoch 23/1000\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5390 - auc: 0.7958 - val_loss: 0.5524 - val_auc: 0.7816\n",
      "Epoch 24/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5388 - auc: 0.7960 - val_loss: 0.5522 - val_auc: 0.7817\n",
      "Epoch 25/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5387 - auc: 0.7961 - val_loss: 0.5521 - val_auc: 0.7820\n",
      "Epoch 26/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5386 - auc: 0.7961 - val_loss: 0.5520 - val_auc: 0.7820\n",
      "Epoch 27/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5382 - auc: 0.7965 - val_loss: 0.5520 - val_auc: 0.7818\n",
      "Epoch 28/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5381 - auc: 0.7967 - val_loss: 0.5520 - val_auc: 0.7819\n",
      "Epoch 29/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5381 - auc: 0.7966 - val_loss: 0.5520 - val_auc: 0.7819\n",
      "Epoch 30/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5381 - auc: 0.7967 - val_loss: 0.5521 - val_auc: 0.7818\n",
      "Epoch 31/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5381 - auc: 0.7966 - val_loss: 0.5520 - val_auc: 0.7819\n",
      "Epoch 32/1000\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.5381 - auc: 0.7966 - val_loss: 0.5521 - val_auc: 0.7817\n",
      "Fold 5 NN: 0.78326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n",
    "\n",
    "test_predictions_nn = np.zeros(test_data.shape[0])\n",
    "\n",
    "\n",
    "scores_folds = {}\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds['NN'] = []\n",
    "counter = 1\n",
    "\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kf.split(X, y)):\n",
    "    print(f'Training fold {fold + 1}')\n",
    "    X_train, X_test = X.iloc[trn_ind][:], X.iloc[val_ind][:]\n",
    "    y_train, y_test = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "    print('CV {}/{}'.format(counter, n_folds)) \n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "    model.fit(X_train, \n",
    "              y_train, \n",
    "              epochs=1000, \n",
    "              batch_size=2048, \n",
    "              validation_data=(X_test, y_test), \n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=1000,\n",
    "              shuffle=True,\n",
    "              verbose = 1)\n",
    "    \n",
    "    preds = model.predict(X_test).reshape(1,-1)[0]\n",
    "    score = round(roc_auc_score(y_test, preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, 'NN', score))\n",
    "    scores_folds['NN'].append(score)\n",
    "    test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds       \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715a30a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:20:46.369094Z",
     "iopub.status.busy": "2021-11-01T17:20:46.368480Z",
     "iopub.status.idle": "2021-11-01T17:20:46.511457Z",
     "shell.execute_reply": "2021-11-01T17:20:46.511964Z",
     "shell.execute_reply.started": "2021-11-01T17:12:34.227207Z"
    },
    "papermill": {
     "duration": 0.549953,
     "end_time": "2021-11-01T17:20:46.512176",
     "exception": false,
     "start_time": "2021-11-01T17:20:45.962223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub=pd.read_csv(\"../input/tabular-playground-series-nov-2021/sample_submission.csv\")\n",
    "sub['target']=test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e4dde9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:20:47.289892Z",
     "iopub.status.busy": "2021-11-01T17:20:47.288943Z",
     "iopub.status.idle": "2021-11-01T17:20:47.299231Z",
     "shell.execute_reply": "2021-11-01T17:20:47.299645Z",
     "shell.execute_reply.started": "2021-11-01T17:12:34.377996Z"
    },
    "papermill": {
     "duration": 0.400729,
     "end_time": "2021-11-01T17:20:47.299828",
     "exception": false,
     "start_time": "2021-11-01T17:20:46.899099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600000</td>\n",
       "      <td>0.737877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600001</td>\n",
       "      <td>0.747742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600002</td>\n",
       "      <td>0.816306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600003</td>\n",
       "      <td>0.339011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600004</td>\n",
       "      <td>0.751990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539995</th>\n",
       "      <td>1139995</td>\n",
       "      <td>0.795158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539996</th>\n",
       "      <td>1139996</td>\n",
       "      <td>0.847601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539997</th>\n",
       "      <td>1139997</td>\n",
       "      <td>0.576607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539998</th>\n",
       "      <td>1139998</td>\n",
       "      <td>0.696465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539999</th>\n",
       "      <td>1139999</td>\n",
       "      <td>0.779144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    target\n",
       "0        600000  0.737877\n",
       "1        600001  0.747742\n",
       "2        600002  0.816306\n",
       "3        600003  0.339011\n",
       "4        600004  0.751990\n",
       "...         ...       ...\n",
       "539995  1139995  0.795158\n",
       "539996  1139996  0.847601\n",
       "539997  1139997  0.576607\n",
       "539998  1139998  0.696465\n",
       "539999  1139999  0.779144\n",
       "\n",
       "[540000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d473b810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T17:20:48.077013Z",
     "iopub.status.busy": "2021-11-01T17:20:48.076443Z",
     "iopub.status.idle": "2021-11-01T17:20:49.334860Z",
     "shell.execute_reply": "2021-11-01T17:20:49.335560Z",
     "shell.execute_reply.started": "2021-11-01T17:13:55.385411Z"
    },
    "papermill": {
     "duration": 1.652012,
     "end_time": "2021-11-01T17:20:49.335870",
     "exception": false,
     "start_time": "2021-11-01T17:20:47.683858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"nn_submission2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717eac7",
   "metadata": {
    "papermill": {
     "duration": 0.383425,
     "end_time": "2021-11-01T17:20:50.105756",
     "exception": false,
     "start_time": "2021-11-01T17:20:49.722331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 326.530421,
   "end_time": "2021-11-01T17:20:53.586839",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-01T17:15:27.056418",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
